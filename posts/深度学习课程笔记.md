---
title: "《 MACHINE LEARNING 2023 SPRING 》课程笔记"
date: "2023-06-05"
tag: "DeepLearning,学习笔记"
---

> 本文来自于李宏毅老师的深度学习课程
> 《MACHINE LEARNING 2023 SPRING》

# 2/24	正確認識 ChatGPT


### 监督式学习

将数据分为输入与输出两类，让机器寻找其中的联系，即$f(x)=y$,找出`f()`这个func，这种模式受限于数据的容量大小。

### 预训练（自监督式学习）

通过某种方法成对产生输入输出数据（比如diffusion的噪声），并进行训练，这便是 **自监督学习** 。而使用此类方法生成的模型又被称为基石模型（底模），一般我们会将预训练的模型再次进行监督式学习。

### 增强式学习（强化学习）

同样的输出内容，由人类判断好坏（浅层反馈 Shallow Learning），对机器的某个判断进行奖励（Reward）。

## Neural Network

现如今，最受瞩目的 `deep learning` 方向则就是 `Neural Network` 。与其相关的两个新兴问题：`Neural Editing`与`Neural Unlearning`。

## Deep Learning Step
1. define *a set of Function* (Neural Network)
2. Goodness of Function
3. pick the best Function

Neural 形如下图：

![image-20230605101421838](https://cdn.jsdelivr.net/gh/Zhuxb-Clouds/PicDepot/img/202306051014068.png)

如果我们以不同的方式去链接 Neural ，就可以得到不同的 Structures.
Different connection leads to different network structures.

Neural Network **Parameter** $\theta$ : all the wight and biases in the "network" 

### Network Connect Way

#### Fully Connect Feedforward （全连接前馈网络）
![pic](https://cdn.jsdelivr.net/gh/Zhuxb-Clouds/PicDepot/img/202306051037303.png)

Input -> hidden layer/Feature extractor replacing/Softmax function -> output = muti-class Classifier

wight 和 biases 是通过train data去寻找的。

`Sigmoid Function`: $\sigma(z)=\frac{1} {1 + e^-z }$

$z = w*x+b$，随后丢入 Sigmoid function，就得到一个`probability`，它被用于 Logistic Regression （逻辑回归），将变量映射到 $(0,1)$ 中。

![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)

当仅决定了 structure 而没有 wight 和 biases 的时候，则称为 `define a function set` 。 Network 就是一连串的 Matrix 运算。

Deep = Many hidden layer

一般来说，hidden layer越多，训练出来的效果越好。

当设计完成 structure 即一个 function set ，不同的 wight 和 biases 对应不同的 function 。

找到 Network Structure 仅能凭借经验与直觉。

### Total Loss

对于每一个 input data ，我们还有与之相对应的 target data ，一般来说我们会计算每一个 output 与 target 之间的 `Cross Entropy` 让这个数值越小越好。

$C(y,\hat{y})=-\sum_{i=0}^n\hat{y_i}\ln{y_i}$

而将所有 output data 与 target 的 C 相加，就得到了 Total Loss 。

$L=\sum_{n=0}^NC^n$ 

Find a function in function set that minimizes total loss L .

> Any continuous function $f=R^n\rightarrow R^m$ can be realized by a network with **one** hidden layer.

### Gradient Descent (梯度下降)
Gradient : Loss 的等高线方向。

**Network parameter** $\theta=\lbrace w_1,w_2...b_1,b_2... \rbrace$

In step 3,we have to solve the following optimization problem:
$\theta^*=arg min L(\theta)$

$$
\theta^0\Rightarrow Compute \nabla L(\theta^0)\Rightarrow
\Bigg [ \begin{matrix}
 \partial L(\theta^0)/\partial w^1\\
 \partial L(\theta^0)/\partial w^2\\
 \vdots\\
 \partial L(\theta^0)/\partial b^1\\
 \partial L(\theta^0)/\partial b^2\\
\end{matrix} \Bigg]\Rightarrow
\theta^0-\eta \nabla L(\theta^0)=\theta^1
$$

$\eta$则指 Learning Rate ，通过设置不同大小的 learning rate ，让函数更新 wight 和 biases ，达到最小 Loss 

$$
\theta^i=\theta^{i-1}-\eta \nabla C(\theta^{i-1})
$$

#### Adaptive Learning Rate

Learning Rate 应该随着训练次数而减小，如 decay: $\eta^t=\frac{\eta}{\sqrt{t+1}}$
但最好还是给每一个不同的 parameter 一个不同的 learning rate 。

##### Vanilla Gradient Descent
$w^{t+1}\leftarrow w^{t}-\eta^tg^t$ 

w is one wight

##### Adagrad
Divide the learning rate of each parameter by the root mean square of its previous derivatives. 

将个学习率都除以之前算出来的微分的RMS（均方根差），RMS实际上是在模拟w的二次微分

$w^{t+1}\leftarrow w^{t}-\frac {\eta^t}{\sigma^t}g^t$ 

g是偏微分的值，$\sigma^t$ 是过去所有微分值的RMS：

$\sigma^t=\sqrt{\frac{1}{t+1}\sum_{i=0}^t(g^i)^2}$

$\eta^t=\frac{\eta}{\sqrt{t+1}}$

##### Stochastic Gradient Descent

寻常的，我们取Loss通常会计算所有 training sample，然而Stochastic Gradient Descent只选择一个 example $x^n$


#### Feature Scaling

Make different features have the same scaling.

将不同的 features 设置在一个相同的scaling范围中。

![Feature Scaling](https://cdn.jsdelivr.net/gh/Zhuxb-Clouds/PicDepot/img/202306061146541.png)

如 Error Surface 所示，相对于差异较大的x，差异较小的在GD的时候更容易接近最低点。

### Back propagation
To computed the gradient decent efficiently, we use backpropagation.

#### Chain Rule

Rule: $g(x)=y$; $z=h(y)$ 

$\Delta x\rightarrow \Delta y \rightarrow \Delta z \Rightarrow \frac{\mathrm{d}y}{\mathrm{d}x}=\frac{\mathrm{d}z}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}x}$

Loss function: $L(\theta)=\sum^w_{n=1}C^n(\theta)$

其中, $C^n$ 即指输出 $y^n$ 与 Expert $\hat{y}^n$ 之间的distance function.

则w对Total Loss的偏微分则为:
$\frac{\partial L(\theta)}{\partial w}=\sum^N_{n=1}\frac{\partial C^n(\theta)}{\partial w}$


---
title: "《 MACHINE LEARNING 2023 SPRING 》课程笔记"
date: "2023-06-05"
tag: "DeepLearning,学习笔记"
---

> **本文来自于李宏毅老师的深度学习课程**
> 
> **《MACHINE LEARNING 2023 SPRING》**

### 监督式学习

将数据分为输入与输出两类，让机器寻找其中的联系，即$f(x)=y$,找出`f()`这个func，这种模式受限于数据的容量大小。

### 预训练（自监督式学习）

通过某种方法成对产生输入输出数据（比如diffusion的噪声），并进行训练，这便是 **自监督学习** 。而使用此类方法生成的模型又被称为基石模型（底模），一般我们会将预训练的模型再次进行监督式学习。

### 增强式学习（强化学习）

同样的输出内容，由人类判断好坏（浅层反馈 Shallow Learning），对机器的某个判断进行奖励（Reward）。

## Neural Network

现如今，最受瞩目的 `deep learning` 方向则就是 `Neural Network` 。与其相关的两个新兴问题：`Neural Editing`与`Neural Unlearning`。

## Deep Learning Step
1. define *a set of Function* (Neural Network)
2. Goodness of Function
3. pick the best Function

Neural 形如下图：

![image-20230605101421838](https://jsd.cdn.zzko.cn/gh/Zhuxb-Clouds/PicDepot/img/202306051014068.png)

如果我们以不同的方式去链接 Neural ，就可以得到不同的 Structures.
Different connection leads to different network structures.

Neural Network **Parameter** $\theta$ : all the wight and biases in the "network" 

### Network Connect Way

#### Fully Connect Feedforward （全连接前馈网络）
![pic](https://jsd.cdn.zzko.cn/gh/Zhuxb-Clouds/PicDepot/img/202306051037303.png)

Input -> hidden layer/Feature extractor replacing/Softmax function -> output = muti-class Classifier

wight 和 biases 是通过train data去寻找的。

`Sigmoid Function`: $\sigma(z)=\frac{1} {1 + e^-z }$

$z = w*x+b$，随后丢入 Sigmoid function，就得到一个`probability`，它被用于 Logistic Regression （逻辑回归），将变量映射到 $(0,1)$ 中。

![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)

当仅决定了 structure 而没有 wight 和 biases 的时候，则称为 `define a function set` 。 Network 就是一连串的 Matrix 运算。

Deep = Many hidden layer

一般来说，hidden layer越多，训练出来的效果越好。

当设计完成 structure 即一个 function set ，不同的 wight 和 biases 对应不同的 function 。

找到 Network Structure 仅能凭借经验与直觉。

### Total Loss

对于每一个 input data ，我们还有与之相对应的 target data ，一般来说我们会计算每一个 output 与 target 之间的 `Cross Entropy` 让这个数值越小越好。

$C(y,\hat{y})=-\sum_{i=0}^n\hat{y_i}\ln{y_i}$

而将所有 output data 与 target 的 C 相加，就得到了 Total Loss 。

$L=\sum_{n=0}^NC^n$ 

Find a function in function set that minimizes total loss L .

> Any continuous function $f=R^n\rightarrow R^m$ can be realized by a network with **one** hidden layer.

### Gradient Descent (梯度下降)
Gradient : Loss 的等高线方向。

**Network parameter** $\theta=\lbrace w_1,w_2...b_1,b_2... \rbrace$

In step 3,we have to solve the following optimization problem:
$\theta^*=arg min L(\theta)$

$$
\theta^0\Rightarrow Compute \nabla L(\theta^0)\Rightarrow
\Bigg [ \begin{matrix}
 \partial L(\theta^0)/\partial w^1\\
 \partial L(\theta^0)/\partial w^2\\
 \vdots\\
 \partial L(\theta^0)/\partial b^1\\
 \partial L(\theta^0)/\partial b^2\\
\end{matrix} \Bigg]\Rightarrow
\theta^0-\eta \nabla L(\theta^0)=\theta^1
$$

$\eta$则指 Learning Rate ，通过设置不同大小的 learning rate ，让函数更新 wight 和 biases ，达到最小 Loss 

$$
\theta^i=\theta^{i-1}-\eta \nabla C(\theta^{i-1})
$$

#### Adaptive Learning Rate

Learning Rate 应该随着训练次数而减小，如 decay: $\eta^t=\frac{\eta}{\sqrt{t+1}}$
但最好还是给每一个不同的 parameter 一个不同的 learning rate 。

##### Vanilla Gradient Descent
$w^{t+1}\leftarrow w^{t}-\eta^tg^t$ 

w is one wight

##### Adagrad
Divide the learning rate of each parameter by the root mean square of its previous derivatives. 

将个学习率都除以之前算出来的微分的RMS（均方根差），RMS实际上是在模拟w的二次微分

$w^{t+1}\leftarrow w^{t}-\frac {\eta^t}{\sigma^t}g^t$ 

g是偏微分的值，$\sigma^t$ 是过去所有微分值的RMS：

$\sigma^t=\sqrt{\frac{1}{t+1}\sum_{i=0}^t(g^i)^2}$

$\eta^t=\frac{\eta}{\sqrt{t+1}}$

##### Stochastic Gradient Descent

寻常的，我们取Loss通常会计算所有 training sample，然而Stochastic Gradient Descent只选择一个 example $x^n$


#### Feature Scaling

Make different features have the same scaling.

将不同的 features 设置在一个相同的scaling范围中。

![Feature Scaling](https://jsd.cdn.zzko.cn/gh/Zhuxb-Clouds/PicDepot/img/202306061146541.png)

如 Error Surface 所示，相对于差异较大的x，差异较小的在GD的时候更容易接近最低点。

### Back propagation
To computed the gradient decent efficiently, we use backpropagation.

#### Chain Rule

Rule: $g(x)=y$; $z=h(y)$ 

$\Delta x\rightarrow \Delta y \rightarrow \Delta z \Rightarrow \frac{\mathrm{d}y}{\mathrm{d}x}=\frac{\mathrm{d}z}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}x}$

Loss function: $L(\theta)=\sum^w_{n=1}C^n(\theta)$

其中, $C^n$ 即指输出 $y^n$ 与 Expert $\hat{y}^n$ 之间的distance function.

则w对Total Loss的偏微分则为:
$\frac{\partial L(\theta)}{\partial w}=\sum^N_{n=1}\frac{\partial C^n(\theta)}{\partial w}$

### Regression 回归

> $x^1$ 上标代表编号
>
> $x_1$ 下标代表feature

#### Step 1: Model

A set of function.

$y=w*x+b$ (linear function)

$y=\sum w_ix_i+b$

#### Step 2: Goodness of function

设有Data组$(x^1,\hat{y}^1)\cdots(x^{10},\hat{y}^{10})$，由此推断 function set ，需判断 function set 好坏。

定义一个 Loss function L : 

L(a function of function set) => how bad it is

$L(f)=L(w*x+b)=\sum^{10}_{n=1}(\hat{y}-(b+w*x^n))^2$ 

(这里为啥要用2次方)


#### Step 3: Gradient Descent

$w^*=arg minL(w)$

1. pick a random $w^0$ value
2. compute $\frac{dL}{dw}|_w=w^0\Rightarrow w^1\leftarrow w^0-\eta\frac{dL}{dw}|_{w=w^0}$

但存在 local optimal 而非 global optimal 的问题。

$$
\nabla L=\Bigg [ \begin{matrix}
 \partial L/\partial w\\
 \partial L/\partial b\\
\end{matrix} \Bigg]_{Gradient}
$$
OverFitting (过拟合)：在不断选择复杂Model的同时，Loss在训练集上下降，而在测试集上提升。
$$
L=\Sigma_n(\hat{y}^n-(b+\Sigma w_ix_i))^2+\lambda\Sigma(w_i)^2
$$

### Classification 分类

$x\rightarrow function \rightarrow Class\space N$ 

Q:为什么不能以Regression的方式做Classification？

A:若样本偏差过大，function会偏向一边来寻求最小的方差。因此Regression good function的定义在Classification不适用。

#### Ideal Alternative

1. function(Model): 
$x\rightarrow\begin{cases}g(x)&\text{Class 1} \\else &\text{Class 2}\end{cases}$
2. Loss function : $L(f)=\sum_n\delta(f(x^n)\neq\hat{y}^n)$
3. find best function : Perceptron,SVM

#### Gaussian distribution

$$
f_{\mu\Sigma}(x)=\frac{1}{2\pi^{\frac{1}{2}}}\frac{1}{|\Sigma|} exp\lbrace -\frac{1}{2}(\frac{x-\mu}{\Sigma})^2 \rbrace
$$

>mean $\mu$ 
>
>covriance matrix $\Sigma$

对每一个Sample进行`Maximum likelihood 极大似然值估计`计算，可得出：
$L(\mu,\Sigma)=f_{\mu\Sigma}(x^1)\dots f_{\mu\Sigma}(x^n)$，
即可能性最大的Function

$(\mu^*,\Sigma^*)=arg maxL(\mu,\Sigma)$

多个class的时候，将多个Gaussian分布的$\Sigma$定为同一个Matrix，即$L(\mu_1,\mu_2,\dots\mu_n,\Sigma)=f_{\mu_1,\Sigma}(x^1)\dots f_{\mu_1,\Sigma}(x^n) f_{\mu_2,\Sigma}(x^{n+1})\dots$

当$\mu_1=\mu_2$时，L最大。$\Sigma=\frac{n}{n+m}\Sigma_1+\frac{m}{n+m}\Sigma_2$

机率模型的选择：经验与直觉

Gaussian：feature之间存在内在联系，而非independence。

### Logistic Regression 

|        | Logistic                                                     | linear                                                       |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Step 1 | $f_{wb}(x)=\sigma(\sum_iw_ix_i+b)$ <br />out: between [0,1]  | $f_{wb}(x)=\sum_iw_ix_i+b$ <br />out: any value              |
| Step 2 | Traning Data: $(x^n,\hat{y}^n)$ <br /> $\hat{y}^n$ :Classification<br />$L(f)=\sum_nC(f(x^n),\hat{y}^n)$ | Traning Data: $(x^n,\hat{y}^n)$ <br /> $\hat{y}^n$ : Real number<br />$L(f)=\frac{1}{2}\Sigma(f(x^n),\hat{y}^n)^2$ |
| Step 3 | $w_i\leftarrow w_i-\eta\Sigma_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$ | Like Left                                                    |

`Cross Entropy`在Logistic Regress 上作为梯度下降的效率胜于Square Error。



| Generative Model (生成模型) | Discriminative Model (识别模型) |
| --------------------------- | ------------------------------- |
| 受Data影响小                | 受Data影响大                    |
| 有预先假设                  | neural network                   |

#### Multi-class Classification

![20230618204117304](https://jsd.cdn.zzko.cn/gh/Zhuxb-Clouds/PicDepot//img/image-20230618204117304.png)

$C_1=w^1,b^1$  $z_1=w^1x+b^1$ 

$C_2=w^2,b^2$  $z_2=w^2x+b^2$

$C_3=w^3,b^3$  $z_3=w^3x+b^3$

##### soft-max

$y_i=P(C_i|x)\Rightarrow D(0,1),\sum_iy_i=1$

在$y_i$与$\hat{y}^i$之间做cross entrpy，$-\sum^3_{i=1}\hat{y}_i\ln y_i$，当Logistic Regression不断对feature进行变换，让model连结成Network，即是Neuro Network，这便是Deep Learning。

## Network 架构设计

### Convolution Neural Network (CNN)

在人类的观察中有这样的现象：某张图片如果含有数个特征符合某物，我们便认为此图片内含某物。CNN的思想与其如出一辙，它定义一个Receptive Field对图片进行特征提取。

Simplification Typical Setting

1. All Channels
2. Kernel Size - 3x3
3. Stride 2

#### Params Sharing

Params Sharing即是在不同的 Neural 里使用相同的wight，从CNN的角度看，就是对不同区域进行特征提取。

#### Benefit of Convolution layer

在使用CNN之后，相比起Fully Connect Layer弹性变小了，而Params Sharing进一步降低了弹性，然而这并不是一件坏事，CNN在具有较高Model Bias的情况下Overfitting的可能性 降低了。

#### 另外一个故事

从另外一个角度来看CNN，每一个Filter对图片进行inner product，形成一个高维的Feature Map，在不断通过Convolution之后，图片会rescale。

#### Pooling - Max Pooling

通常来说，Convolution之后会接Pooling，即选定一个范围之后按照某种规则选择范围内最符合规则的Feature去代表这个范围。池化的目的是快速缩小图片、减少运算量。

$Img \rightarrow Convolution \rightarrow Pooling\rightarrow Full Connect$

### Self-attention

当Vector as Input，对于Input、Output大小长度不同，可分为三种类型。

1. N->N
2. N->one label
3. N->N'

自注意机制用于面向Seq2Seq，即Input、Output动态大小的情况。使用Self-attention对vector进行预处理，将Content的内容加权到本身的Vector上。Self-Attention对于某一个Vector，会注意它与content其他vector之间的相关性，如：$v_1w_k \cdot v_2w_q=\alpha$(dot-product),或$tech(v_1w_k \cdot v_2w_q)W=\alpha$(additive)

注意:v1也会与自己计算相关性

如果将Self-attention的Input看作一个整体的矩阵,那便是:$O=K^tIw^q$

#### Multi-head Self-attention

different type of relevance，乘以不同的矩阵，得出结果再相加。

#### Position Encoding

self-attention无法标识将先后顺序与位置信息，就需要使用Position Encoding。

| Self-attention | CNN            |
| -------------- | -------------- |
| 考虑全图       | 只考虑Filter   |
| 需要更多Data   | 只需要少量Data |

CNN是一个Self-attention的特例，Self-attention则是考虑全图的CNN

### Transform

transform是为了解决Seq2Seq问题的模型架构，而Seq2Seq可以用在各种问题上，作为一种通解。

$Input\rightarrow Encoding \rightarrow Decoding \rightarrow Output$

![image-20230628191501262](https://jsd.cdn.zzko.cn/gh/Zhuxb-Clouds/PicDepot/img/202306281917734.png)

#### Encoder

Encoder是一个输入、输出同样长度向量的 Block ，在Transform中，Encoder就是Self-attention。

#### Residual Connection 残差链接

即在经过Self-attention层之后的x‘再加上原本的input，输出$y=f(x)+x$,它用于解决梯度消失、梯度爆炸等问题。

再接着，对 Residual 作 Norm $\hat{x_i}=\frac{x_i-m}{\sigma}$

#### Masked Self-attention

在Decoder当中，由于输出值是通过不断循环而推理出的，Self-attention不能在一开始就知道后面的token和自己的相关性，因此在这里是Masked的。

#### AutoRegressive

单看Decoder，其实它做着这样一件事：不断将output变成下一次的input，以这样类似于RNN的方式输出Seq。我们通过Begin token让Decoder知道从何时开始，而它通过输出End让我们知道在哪里结束，而Input持续不断的在Mask Muti-head Attention层影响Decoder。

#### Not AutoRegressive (NAT)

与AT不同的是，NAT并不会让Decoder循环输出，而是一次性完成，它设定一个输出的最大长度，然后每一个Token都使用Begin Token占位。它的好处在于并行与控制，在语音生成TTS中，NAT可以通过控制Token数量长度来控制语速。

How to decide the output length of NAT decoder?

* Another Predictor from Encoder.
* A maxium Length,ignore tokens after END token.

Performance: NAT < AT (because "Muti-modality")

#### Exposure bias

它是指一种现象：当Test的时候，Decoder一旦生成出局部错误内容会导致整体性的错误，这是因为Train数据的绝对正确。

#### Scheduled Sampling

针对Exposure bias，有目的的在Train中加入错误的数据，这被称为Scheduled Sampling。

## FineTurning vs. Prompting

| FineTurning | Prompting     |
| ----------- | ------------- |
| 专才        | 全才          |
| 通过Adapter | 通过Prompting |

### 各种Adapter

![image-20230628212431011](https://jsd.cdn.zzko.cn/gh/Zhuxb-Clouds/PicDepot/img/202306282135481.png)

1. Bitfit：只微调Neural的bias
2. Houlsby：在Feed-Forward后面再插入一个Feed-Forward
3. adapter bias：与Feed-Forward平行，对Feed-Forward作平移等操作
4. Prefix/Lora：更改Attention参数

## 影像生成

现行的影像生成总是在Text$\rightarrow$Image中加入一个从Normal Distribution里Sample出来的Vector，我的理解是这个Distribution是对真实图片的Distribution线性变换后产生的有限空间。

1. VAE：Vector来自于Encoder对Image的编码。
2. Flow-Base：Encoder与Decoder inverted，在通过Encoder对图像编码自然产生Vector。
3. diffusion model：将Vector视作真实噪音，仅通过add noise的方法生成Vector，将注意力放在denoise上。
4. GAN：再设计一个discriminator与decoder（genarate）进行对抗。

### Denising Diffusion Probabilistic Model (DDPM)

#### Algorithm1 Training

1. repeat
2. $x_0\sim q(x_0) \leftarrow$sampling clean image
3. $t\sim Uniform(\{1,\cdots,T\})$
4. $\epsilon \sim \mathcal N(0,I) \leftarrow$Sampling a noise
5. Take gradient desent step on:$\nabla_\theta\Vert \epsilon-\epsilon_\theta(\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon,t)\Vert^2$

与很多人想象的不同的是，DDPM在训练的时候，一次就将噪音加入Image中，并将它作为输入，让Denoise去预测生成的噪音。

#### Algorithm 2 Sample

1. $x_T\sim\mathcal N(0,I)$
2. for $t=T,\cdots,1$ do
3. $z\sim\mathcal N(0,I)$ if $t>1$ ,else $z=0$
4. $x_t-1=\frac{1}{\sqrt{\overline\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline\alpha_t}}\epsilon_\theta(x_t,t)+\sigma_tz)$
5. end if
6. return $x_0$

## 目前影像生成的目标

在一个已有的简单分布中，Sample一个z，在通过Denoise之后变成一张图片，这张图片实际上是在一个极其复杂的分布中。而影像生成的目标就是让这个分布与真实图片的分布越接近越好。

